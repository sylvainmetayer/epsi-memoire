\subsection{Gestion de projet}

\epigraph{\frquote{Avoir 300 personnes dans une cuisine pour faire un œuf au plat ne permettra pas de servir le plat 300 fois plus vite}}{Analogie de la \frquote{Loi de Brooks\footnote{\url{https://fr.wikipedia.org/wiki/Loi_de_Brooks}}}}

\subsubsection{Partie financière}
L'automatisation d'un projet ne peut s'effectuer un en jour. De même que la citation ci-dessus fait sens, la migration d'un projet existant vers une automatisation totale ne peut s'effectuer en un jour, même en mobilisant de grand moyens. Il est donc nécessaire de préparer et planifier une conduite du changement qui permettra d'accompagner tous les acteurs du projet, afin que la transition s'effectue dans les meilleurs conditions possible. Cela passe d'abord par une réflexion sur la conduite à adopter et les changements à apporter. 

Prenons pour exemple un projet fonctionnel depuis maintenant plusieurs mois. Le projet a déjà été livré (avec plus ou moins de difficulté) quelques fois en production. Comment convaincre que l'automatisation va apporter du bien à ce projet ? Qui est-il nécessaire de convaincre ?

Tout d'abord, il faut convaincre les personnes responsables financièrement du projet. En effet, il est possible d'avoir un projet automatisé de A à Z, si ce processus d'automatisation vient à coûter 3 fois le prix total vendu au client, aucun responsable ne vous donnera son accord. Il va également falloir convaincre les personnes responsables du projet (chef de projet, directeur de projet...) qu'il y a un intérêt à effectuer cela. En effet, sans garantie que ce processus pourra amener quelque chose de positif, peu laisseront une opportunité d'essayer d'automatiser un projet.

Il faut ainsi préparer son argumentaire, avec les bienfaits que cela pourra obtenir sur les \gls{KPI} et sur la vie du projet à moyen/long terme.

\begin{description}
	\setlength\itemsep{0em}
	\item [La durée de de déploiement] peut être réduite pour permettre de se concentrer sur d'autres problématiques. De plus, si les temps de déploiement sont réduits, cela signifie que les temps d'interruption de service pourront être réduit lors des livraisons.
	\item [Erreur de déploiement] Si les déploiements sont automatisés, les risques d'incidents lors des déploiements, souvent du à des erreurs humaines, seront alors réduit.
	\item [Rentabilité financière] Prenons une tâche qui coûte \numprint{3000}€ à réaliser toutes les 3 semaines. Son coût est donc assez élevé. Le fait d'automatiser cette tâche permettra d'économiser de l'argent sur la durée. Si le fait d'automatiser cette tâche coûte par exemple, \numprint{10000}€, un gain sera perçu dès la 12\up{ème} semaine.
	\item [Part de marché] Si les déploiements sont plus fiables et rapides, que la rentabilité financière du projet est également florissante alors il sera possible de concentrer les efforts sur des fonctionnalités à forte valeurs ajoutées qui permettront ainsi de se démarquer de la concurrence et d'augmenter les parts de marché à moyen/long terme.
	\item [\Gls{timetomarket}] Dans un environnement très concurrentiel, il est essentiel de pouvoir intervenir rapidement. Réduire la durée entre la naissance d'une idée/besoin et sa mise en production est important, sous peine de voir des concurrents prendre des parts de marché. L'automatisation peut alors permettre de déployer rapidement et avec confiance les développements réalisés.
	\item [Reprise d'activité] Il s'agit ici de démontrer que le produit parfait n'existe pas. Il arrivera que l'application soit hors ligne à cause d'un incident ou qu'une mise à jour entraine une interruption de service. En revanche, si l'on ne peut éviter l'incident à un moment donné, il est capital de pouvoir restaurer l'application dans des court délai et ce de façon certaine. Disposer d'un \gls{PCA} ou d'un \gls{PRA} intégrant une automatisation de la reprise d'activité est un atout non négligeable qui permettra une reprise de l'activité au plus vite. On peut alors passer d'une interruption de plusieurs heures à une se comptant en minutes.
\end{description}

\subsubsection{Organiser le flux de travail}
Il est également nécessaire de planifier au mieux les durées des travaux à accomplir afin d'éviter des dépassements dans les délais annoncés. Cela implique donc de réfléchir au temps nécessaire à investir et aux différents jalons qui valideront la bonne mise en place de l'automatisation. 

Pour pouvoir mettre en place une démarche d'automatisation sur un projet, il est nécessaire que ce dernier soit assez mature pour pouvoir s'y adapter. Ainsi, comme l'indique \citetitle{phoenixProject} \cite{phoenixProject},  avant de se lancer dans une démarche d'automatisation (\emph{ou plus globalement, pour pouvoir gérer correctement un projet}), la première étape est de savoir gérer le flux de travail.

Le flux de travail, ce sont les différentes tâches qui sont à réaliser sur le projet. Ces dernières sont souvent des stories (\emph{dans le cas d'une méthodologie agile}), définies conjointement avec le client et correspondent ainsi à ce qui a été défini dans le cahier des charges. L'inconvénient est que ce flux de travail est fragile. Il peut rapidement dériver et se retrouver surcharger, surchargeant ainsi les équipes. Le principal ennemi de flux de travail est le travail non prévu (\emph{unplanned work}). Ce sont toutes les petites tâches non prévues, qui prennent 10 à 15 minutes, qui ne sont suivies nulle part et qui mises bout à bout peuvent faire perdre plusieurs jours de travail et épuiser les équipes, qui ne voit jamais le travail diminuer.

Quelles sont donc les solutions pour gérer ce travail non prévu ? L'objectif est ici de faire en ce sorte que ce travail non prévu ne soit plus exécuté directement sans passer par une phase minimale de planification. Pour cela, il est possible de mettre en place des kanbans. Qu'est ce qu'un Kanban ? Il s'agit d'une méthode de représentation des tâches qui permet de visualiser l'avancée du projet en temps réel (\emph{ou du moins, à chaque fois que le tableau est mis à jour}). On peut ainsi limiter le travail non prévu, en l'ajoutant à une catégorie dédiée, le \emph{backlog}, qui correspond à la pile de tâche à réaliser. Une fois ces tâches estimées, découpées et discutées avec le client, elles peuvent être considérées comme prête à être commencées. On parle alors de \emph{Définition Of Ready}. De même, pour déterminer qu'une tâche a été terminée par l'équipe de développement et donc prête à être livrée, on parle de \emph{Definition Of Done}, ce qui correspond à une liste de points, établi par l'équipe en interne. Ces points correspondent à des éléments à vérifier et à cocher pour indiquer si une tâche est vraiment terminée ou non. On peut y retrouver par exemple l'écriture de tests ou encore le fait qu'une autre personne ai relu le code, le fait que la documentation ait été mise à jour\ldots

%\newImage{0.35}{kanban.jpg}{Représentation d'un tableau Kanban -  \url{https://commons.wikimedia.org/}}{kanban}

On peut ensuite suivre l'évolution des tâches dans leur différentes étapes, du développement à leur livraison, en passant par les tests. Ainsi, le travail non prévu ne peut plus déranger un sprint en cours puisqu'il est possible de définir des limites, en disant par exemple que seules les tâches étant prêtes seront travaillées dans ce sprint et que les autres devront attendre le prochain sprint pour pouvoir être traitées. Néanmoins, il s'agit de faire attention à un point en mettant en place un kanban. En effet, si l'on met simplement en place un kanban et que l'on continue de traiter les tâches comme elles l'étaient précédemment, on s'expose au risque de voir le travail en cours, ou \emph{WIP\footnote{Work In Progress}} s'accumuler. Cela risque donc de ralentir les équipes voir de les démotiver. De plus, si tous le travail est en WIP, il n'y a aucune façon de savoir depuis combien de temps un tâche est présente ici, ce qui fait que certaines tâches peuvent très bien rester en WIP pendant des jours voir des semaines. La solution à ce problème tient dans le fait de limiter le travail en cours. En effet, en limitant le travail en cours, on s'assure que les tâches ne resteront pas éternellement en attente. Les tâches bloquées pourront éventuellement être passées dans une colonne "Bloqué"\footnote{Cette colonne devra elle aussi être limitée en éléments, sous peine de seulement décaler le soucis !}. De plus, si le travail en cours est restreint, il sera de meilleur qualité, puisque moins dispersé et donc engendrera moins de bug !

\subsubsection{Livrer plus rapidement de la valeur métier}
L'un des objectifs de l'automatisation est également de faciliter le passage d'un système où l'on met plusieurs mois avant de livrer une grosse fonctionnalité à un autre où l'on livre de multiples petites fonctionnalités. Attendre plusieurs mois pour livrer une fonctionnalité peut en effet avoir des effets indésirables.

\begin{itemize}
	\setlength\itemsep{0em}
	\item Dans une époque hyper-concurrentielle où l'on veut tout, toujours plus vite, certains besoins sont éphémères ou nécessite un \gls{timetomarket} faible pour se démarquer de la concurrence. Si l'on livre la fonctionnalité une fois le besoin passé, la fonctionnalité n'a plus de sens. Cela vaut aussi si l'on vient à livrer un produit une fois que tous les concurrents sont déjà positionnés dessus. Il vaut mieux livrer petit à petit des parties du besoin pour pouvoir l'améliorer continuellement.
	\item Une fonctionnalité qui met longtemps à être livrée induira un stress au niveau des équipes, qui auront sur leurs épaules la responsabilité de livrer \emph{la fonctionnalité parfaite}, plutôt que livrer plusieurs petites fonctionnalités.
	\item Le risque d'échec est beaucoup plus important en livrant une fonctionnalité plutôt que plusieurs petites, surtout si des changements d'architecture (\emph{Structure de la base de données, ajout de nouveaux composants...})
	\item Plus l'on attend longtemps pour livrer une fonctionnalité, plus on prend le risque de se retrouver dans le cas d'un \emph{effet tunnel} ou la compréhension de ce qui était à faire s'est retrouvée au fur et à mesure du temps par être l'opposé de ce qui était demandé. Dans cette position, le client et le chef de projet communique souvent peu, l'équipe de développement travaille en général sans avertir de ses avancées,\ldots Seule la date limite permet de voir ce qui a été réalisé et il s'agit plus souvent d'un échec que d'une réussite.
\end{itemize}

Cette volonté de vouloir livrer plus petit mais plus régulièrement s'inscrit dans \emph{l'amélioration continue}. Au début, la fonctionnalité ne sera peut-être pas parfaite, mais elle sera continuellement améliorée jusqu'à correspondre au besoin voulu. 

\newImage{1}{devops-objective.jpg}{Mise en place de l'amélioration continue entre \emph{Dev} et \emph{Ops} - \url{https://itrevolution.com}}{devopsObjective}

Néanmoins, il convient de vérifier que ce système est intégré et accepté par l'équipe, sous peine de voir un rejet de cette méthodologie et d'entrainer des conflits internes dans les équipes. Cela peut être le cas avec certaines personnes réfractaires au changement, par exemple des développeurs âgés, proche de le retraite, qui n'ont peut-être pas envie de se reformer et s'adapter à de nouvelles méthodes de travail et qui tenteront de garder leurs habitudes. Cela peut également provenir de personnalités avec un fort caractère qui pensent que leur méthode est plus efficace que les autres ou encore de bien d'autres cas. De manière générale, les personnes ne souhaitent pas sortir de leur zone de confort. Pour permettre ce changement, il va falloir exposer les différents avantages aux équipes et leur montrer les intérêts qu'ils ont à s'essayer à cette méthode pour peut-être finir par l'adopter de leur plein gré. 

Afin de faire intégrer ce système par une équipe, il faut tout d'abord trouver des personnes qui sont ouvertes au changement, des \glsplural{earlyadopter} qui ont envie de voir cette amélioration se mettre en place. Dans l'idéal, ces personnes seront également des personnes influentes vis-à-vis des autres afin de permettre une meilleure intégration du changement par le suite. Il faut ensuite convaincre une majorité de personne et les amener vers ce changement. L'idée est de construire une base plus importante de personnes souhaitant voir cette évolution se mettre en place afin de ne laisser que les personnes réfractaires au changement à gérer par la suite, quitte à les mettre devant le fait accompli une fois une majorité de personnes convaincue. On peut voir cette mise en place de la culture \emph{DevOps} détaillée dans \citetitle{devOpsHandbook} \cite[p.58-59]{devOpsHandbook}.

\subsubsection{Retours et apprentissage continu}

L'un des premiers avantages si l'on met en place une démarche d'automatisation est une meilleure communication entre les développeurs, également appelés \emph{Dev}, et les administrateurs systèmes, appelés \emph{Ops}. Si l'on met en place une démarche d'automatisation, un besoin de métrique va être nécessaire. Ces métriques, semblables aux \gls{KPI} précédemment détaillés, permettront de savoir par exemple le temps de déploiement de l'application, les derniers bugs remontés, les suivis de performance afin de déterminer si une requête \gls{SQL} est trop lente ou non... Ces métriques, conjointement définies par les \emph{Devs} et les \emph{Ops} vont permettre d'obtenir des retours rapides sur l'état de l'application et ainsi de pouvoir améliorer le produit plus facilement, en corrigeant les bugs au plus tôt. C'est la base de la \emph{Second Way}, ou deuxième étape, comme expliqué dans \citetitle{phoenixProject} \cite[p.405-410]{phoenixProject}.

Un autre point à prendre en compte lors d'une démarche d'automatisation est l'apprentissage continu. Comme nous l'avons vu dans la partie précédente, \etsy a mis en place un outil de post-mortem (p.\pageref{post-mortem}) qui permet de savoir les erreurs rencontrées dans le passé et de permettre qu'elles ne se reproduisent plus. Cette notion constitue la \emph{Third Way} mais ne se limite pas seulement aux erreurs rencontrées. Avec le temps obtenu par l'automatisation, il est ainsi possible de s'améliorer sur d'autres points : partager les connaissances de chacun pour faire progresser l'équipe et les personnes la constituant, documenter et améliorer l'intégration de nouvelles personnes dans les équipes ou les environnements de travail\ldots{} L'équipe sera alors plus performante et pourra ainsi délivrer de meilleurs produits.

% TODO Le temps libéré par l'automatisation des tâches peut permettre de souder les liens d'une équipe et d'améliorer les relations de cette dernière. Cela libère du temps pour du team building par exemple. Un sujet technique qui rapproche en terme d'humain

\subsection{Environnement de développement}

L'environnement de développement est le premier environnement où sont développées les différentes fonctionnalités du projet. Ce cadre de travail doit pouvoir être installé rapidement afin de se concentrer sur le développement de fonctionnalités et non sur l'installation de l'environnement de travail.

\subsubsection{Arrivée sur le projet}

Plusieurs personnes sont potentiellement amener à travailler sur le projet. Il est donc important de documenter l'installation et les pré-requis afin de pouvoir commencer à travailler. Il n'est malheureusement pas rare de voir des projets où la documentation du projet est faible, voir inexistante.

L'idéal pour un développeur qui vient d'arriver sur un projet serait de disposer d'un environnement prêt à l'emploi, qui puisse se lancer en quelques commandes (installation des dépendances et provisionnement de l'environnement). Des outils de conteneurisation tel que Docker\footnote{\url{https://www.docker.com/}} permettent ainsi d'avoir des environnements isolés et reproductibles quelque soit l'environnement du développeur. De plus, avec Docker, l'architecture du projet est représenté sous forme textuelle et donc intégrable dans le \gls{SCM}, ce qui permet de savoir les évolutions sur l'architecture du projet facilement. On parle alors d'\gls{IaC}. De plus, un projet qui peut s'installer rapidement signifie qu'un développeur peut être efficient plus rapidement sur son projet. Il peut ainsi apporter ses compétences sur de réelles problématiques plutôt qu'être bloqué pendant de longues heures à chercher comment installer ledit projet.

Afin de permettre un démarrage rapide, il est également commun de mettre des alias sur les commandes les plus utilisées lors des développements. Par exemple, si une commande permettant d'effacer le cache de l'application est longue à taper, on la raccourcira en quelques chose tel que \frquote{make clear} par exemple. Pour cela, plusieurs solutions sont disponibles: les scripts \gls{PHP} dans le cas d'un projet \gls{PHP} ou encore l'utilisation des Makefile, permettant de construire les logiciels et leur dépendances et d'y effectuer des tâches diverses, définies dans un fichier. Il est également possible d'écrire des scripts shell, permettant d'effectuer des tâches plus complexe\footnote{Comme faire le café ? \emoji{😊} - \url{https://github.com/NARKOZ/hacker-scripts}}

De plus, il est souvent nécessaire d'avoir des configurations spécifiques sur les environnements de développements. En effet, dans un contexte de développement, on va vouloir obtenir les erreurs immédiatement, dans un terminal par exemple, alors que dans un environnement de production, ces dernières seront dans un fichier de log. De plus, on va pouvoir vouloir tracer le temps que prend chaque requête vers une base de données ou encore disposer d'outils permettant de mettre des points d'arrêts dans le code afin de pouvoir débugger lors des phases de développements. Il faut donc des moyens d'activer ses outils simplement. Cela peut passer par des scripts ou encore des images Docker dédiées au développement, basée sur celle de production. Le but est d'obtenir un environnement le plus proche possible de la production tout en ayant la possibilité d'ajouter des outils de développements additionnels.

\subsubsection{Build local}

Le build local correspond à la première boucle de retour au développeur. C'est dans cet environnement qu'il obtient des retours rapide, que cela soit sur ses tests unitaire, ses éventuelles fautes de frappe, erreur de syntaxe\ldots. L'objectif de ce build local est de détecter les erreurs au plus tôt, pour éviter qu'elle ne soient potentiellement oubliées et non détectées par les autres étapes du flux de travail et provoque un bug en production alors qu'il aurait pu être évité simplement (dans le cas d'une erreur de syntaxe). Plusieurs solutions peuvent alors être mise en place pour améliorer ce build local.

\paragraph{L'utilisation d'un IDE}

L'objectif de l'\gls{IDE} est de fournir un support de travail au développeur et de lui repérer de façons automatique des erreurs d'inattention. Il convient alors de se familiariser avec un \gls{IDE} afin de tirer le maximum de ce dernier. Parmi les fonctionnalités communes d'un bon \gls{IDE}, on retrouve :

\begin{itemize}
	\setlength\itemsep{-0.5em}
	\item Détection des erreurs de syntaxe
	\item Ajout automatique des imports dans un fichier
	\item Lancement automatique des tests unitaire à la sauvegarde/modification d'un fichier
	\item Auto-complétion automatique permettant de connaître le nom des méthodes et fonction à la saisie
	\item Indenter le code selon des conventions définies
	\item Affichage des erreurs communes simple à détecter (tel qu'un code non atteignable car après un retour de fonction par exemple)
\end{itemize}

Les \gls{IDE} disposent souvent de plugins qui permettent d'étendre leurs fonctionnalités pour pouvoir améliorer le quotidien du développeur. Par exemple, certains plugins vont permettre de gérer l'affichage de l'état de la base de données, s'interfacer avec l'état des outils d'intégrations continue distants (via les \gls{API} de ces derniers) afin de savoir si les tests d'intégrations sont passés ou non\ldots

\paragraph{Les hooks du SCM} Git, \gls{SCM} utilisé de façon majoritaire dans les développements dispose d'un mécanisme permettant d'étendre ses fonctionnalités : les hooks. Ces hooks, tel que décrits dans la documentation (\citetitle{git-hook} \cite[chapitre 8.3]{git-hook}) permettent de lancer des scripts à certaines étapes, tel qu'avant un commit, avant de pousser le code sur le serveur, après réception des modifications du serveur\ldots{} Cela permet souvent de vérifier que les tests unitaires passent, qu'il n'y a pas d'erreur de syntaxe ou encore que le linter\footnote{analyse de code statique permettant de détecter les erreurs communes, et d'instaurer une syntaxe (règles d'espacement, nommage des variables\ldots) commune} ne génère pas d'erreur

\paragraph{Le pair programming (programmation en binôme)}

Le pair programming consiste à développer à deux développeurs sur le même poste de travail. Bien qu'au premier abord, cela puisse sembler être une perte de temps et d'argent (puisqu'il n'y a au final qu'une seule personne qui tape sur le clavier), cette méthode permet de détecter des erreurs plus tôt, puisqu'il y a 2 paires d'yeux qui peuvent relever les erreurs au lieu d'une seule. Souvent, il peut arriver un développeur seul d'être \frquote{la tête dans le guidon}, c'est à dire qu'il est tellement absorbé dans son code et peut donc oublier certains cas, certaines règles métiers\ldots{} Le fait de travailler en binôme permet ainsi de déceler ces erreurs plus tôt et d'assurer une meilleure qualité de code. Les deux développeurs échangent rapidement leur rôles afin de travailler de façon efficace et de passer d'observateur à développeur. 

\subsection{L'importance des tests} \label{importance-test}

Les tests ont un rôle crucial dans une démarche d'automatisation. En effet, lorsque l'on automatise quelque chose, il est essentiel de s'assurer que le comportement de l'application est valide puisque l'on va par la suite déployer automatiquement cette application en production. Selon le \gls{CFTL} et l'\gls{ISTQB}, il existe 4 types de tests et chacun dispose de 4 objectifs formant ainsi une matrice représentée dans le tableau \ref{tab:matrice-test}.


\begin{table}[H]
	\centering
	\begin{adjustbox}{width=0.95\textwidth,center=\textwidth} 
		\begin{tabular}{|l|l|l|l|l|}
			\hline
			\diagbox{type de test}{valide} & Fonctionnement unitaire & Règle métier & Développement & Interaction \\ \hline
			Test de composant     & X                       & X            & X              &             \\ \hline
			Test d'intégration    &                         & X            & X              & X           \\ \hline
			Test Système          &                         & X            &               & X           \\ \hline
			Test d'acceptation    &                         & X            &               &             \\ \hline
		\end{tabular}
	\end{adjustbox}
	\caption{Matrice de tests selon leur type et leur objectif}
	\label{tab:matrice-test}
\end{table}

\subsubsection{Le test de composant}\label{test-composant}

Aussi appelé \gls{TU}, son objectif est que le composant testé soit totalement isolé afin de se concentrer uniquement sur le \emph{fonctionnement unitaire} de ce dernier, sans dépendre d'autre composant. On peut alors tester tous les cas, du cas dit \frquote{général}, qui représente souvent 80\% des cas, jusqu'aux \frquote{cas limites}, cas apparaissant plus rarement mais pouvant entrainer un comportement non souhaité de l'application. Cela permet de vérifier que le code écrit par le développeur lors de la phase de \emph{développement} est bien valide. Ainsi, un test de composant peut valider des \emph{règles métier} et ainsi également correspondre à un test d'acceptation\footnote{détails sur les tests d'acceptation dans la sous partie \frquote{\nameref{test-acceptation}} (p. \pageref{test-acceptation})}.

Un exemple commun de test de composant est par exemple le test d'une méthode de calcul d'une calculette. On dispose d'une méthode \frquote{calculer} disposant de 3 paramètres, 2 \frquote{opérandes} numériques et un \frquote{opérateur}. Un cas général serait alors de tester que \frquote{3+2} retourne bien \frquote{5} et un cas limite serait de tester que \frquote{5/0} lève bien une exception, puisqu'il s'agit d'une opération illégale.

Dans le cas de \gls{PHP}, un exemple de \gls{framework} de test unitaire est PHPUnit\footnote{\url{https://phpunit.de/}}. Le principal avantages de ce type de test est sa rapidité : ne nécessitant aucune interaction avec des composants externes (ou bien ces composants disposent d'un \gls{mock}), l'exécution des tests est très rapide.

\subsubsection{Le test d'intégration}

Une fois chaque composant testé unitairement, il convient de vérifier que ces composants sont capable d'\emph{interagir} entre eux. En effet, dans la sous-partie précédente, il était indiqué que les composants étaient isolés les uns des autres. Néanmoins, dans de nombreux cas, il est nécessaire que des composants interagissent entre eux. On va alors alors utiliser un \gls{mock} afin de simuler le comportement d'un composant, afin de tester l'autre.

Un exemple serait une \gls{API} qui communiquerait avec une base de données afin de retourner les résultats. Lorsque l'on teste l'\gls{API} unitairement, on ne souhaite pas dépendre de la base de données. On va donc simuler les réponses que nous donnerait la base de données et envoyer ces données à l'\gls{API} qui pourra alors être testée.

Cependant, si l'on teste toute notre application de cette façon, il est impossible d'attester le bon fonctionnement de nos composants entre eux. La version de la base de données pourrait en effet être incompatible avec le code actuellement utilisé par l'\gls{API}. Si aucun test d'intégration n'est effectué, la confiance dans l'application sera basée sur le \gls{mock} de la base de données et on risque alors de se retrouver dans le cas de la figure \ref{fig:no-integration-test}, avec des composants isolés fonctionnant très bien individuellement mais incapable de s'exécuter ensemble. Cela s'illustre avec la figure \ref{fig:no-integration-test}. Dans ce cas, les tiroirs s'ouvrent très bien individuellement mais lorsqu'on les assemblent, il n'est pas possible d'ouvrir les deux tiroirs en même temps, puisque le premier bloque l'ouverture du second.

\newImage{0.4}{no-integration-tests.png}{Exemple de tests unitaire sans tests d'intégration.}{no-integration-test}

Les tests d'intégration vont donc garantir que chaque composant, déployé dans l'environnement d'exécution de l'application, est capable de communiquer de façon correcte avec ses dépendances.

\subsubsection{Le test système}

Les tests système permettent de tester le comportement de l'application sans forcément avoir connaissance du code. Cela permet ainsi de vérifier que l'application effectue bien ce qui est indiqué dans les spécifications demandées et que les différents composants interagissent bien entre eux. Ces tests sont également appelés \frquote{tests boite noire}. De plus, les tests systèmes permettent de vérifier que les composants interagissent correctement entre eux, au niveau global de l'application (\emph{au niveau système}) et non en mettant un composant dans l'application et en regardant son comportement avec les autres composants, comme c'est le cas avec le test d'intégration.

Ce sont également les tests qui sont les plus haut niveau, puisqu'il s'exécutent avec toute l'application et ses dépendances / composant. On peut par exemple citer les tests \gls{E2E} qui sont des tests système qui permettent de tester le comportement d'une application en décrivant les actions réalisées sur cette dernière (cliquer sur un bouton, réduire la fenêtre) et les résultats attendus.

Le \gls{framework} \gls{angularjs} , par exemple, permet de réaliser des tests \gls{E2E} avec Protractor\footnote{\url{https://www.protractortest.org/}}. Cela va ainsi émuler un navigateur dans lequel il va être possible de décrire les comportements utilisateurs. 

Un autre exemple de test système serait un test de charge, réalisé sur un environnement similaire à la production, durant lequel on ne va pas tester la conformité de l'application mais sa capacité à supporter un certain trafic. Dans le cas d'une application web, cela consiste à simuler des centaines ou milliers de connexion simultanées sur le site afin de vérifier sa capacité à répondre convenablement. Évidemment, tester ceci sur un environnement avec une configuration différente de la production n'a que peu de sens, puisque le serveur de test sera alors sous-dimensionné, sur-dimensionné ou disposera d'une gestion du cache différente par exemple mais dans tous les cas, les résultats seront faussés.

L'avantage de ces tests systèmes est qu'il s'agit des tests les plus complets, puisqu'ils permettent de tester le produit fini. L'inconvénient est que ces derniers sont plus longs à exécuter.

%\subsubsection{Campagne de test}
%Parler de Protractor et Squash sur \bv
%Parler de Protractor et Squash sur \bv. Tests automatisés unitaires (phpunit) permettant à la plateforme d'intégration continue de vérifier la conformité du code par rapport aux attentes métiers.

\subsubsection{Le test d'acceptation}\label{test-acceptation}

Souvent réalisé par le client, il va consister, au travers d'une campagne de test généralement manuelle, à vérifier que les \emph{règles métiers} sont bien respectées. Le client peut néanmoins décider de déléguer ce test à une équipe qualité, ou à un prestataire externe.

Il est néanmoins possible que certain test d'acceptation soit validé en amont, comme l'exemple de la calculatrice donné dans la partie \frquote{\nameref{test-composant}} (p. \pageref{test-composant}) qui va valider la \emph{règle métier} (effectuer une opération) tout en validant le bon fonctionnement du code et ses cas limites.

\subsubsection{Les tests par mutation et chaos engineering}

\todo[color=gray]{Partie bonus test mutation / chaos engineering à eventuellement traiter}

Ces deux pratiques viennent compléter les batteries de tests existant afin de garantir que le système, et l'application puissent réagir à tout instant, en venant volontairement modifier le code ou endommager un serveur ou paramètre sur le serveur de l'application afin de constater comment réagit cette dernière.

\subsection{Intégration, déploiement et livraison continue}

Une fois que les tests permettent de modifier le code de l'application avec confiance sans introduire de régression, on peut alors mettre en place une démarche d'automatisation. Cela va permettre à l'équipe de développement d'obtenir un retour rapide lors de chaque déploiement, avec un lancement automatisé des tests ainsi qu'un déploiement automatique de l'application sur un environnement de recette interne par exemple.

L'un des objectifs de l'intégration et du déploiement continu est de banaliser le déploiement afin de réduire la peur des développeurs vis-à-vis de la mise en production. Plus le code est déployé souvent, plus les développeurs seront sereins quant à leur travail puisqu'ils pourront obtenir des retours rapide et être prévenu au plus vite lors d'incident afin de les corriger avant qu'il n'atteigne la production.

\subsubsection{Gestion de version, de branches et des opérations}

Il est tout d'abord essentiel de définir une stratégie de gestion de version et de branches du code, afin de s'assurer une bonne mise en place de l'automatisation. De plus, il est important de réfléchir aux actions qui devront être entrepris dans chaque cas. 

En effet, avec l'automatisation du processus d'intégration, de livraison et de déploiement, il est nécessaire d'adopter en amont des \emph{conventions} afin de couvrir les différents cas qui pourraient survenir. Les actions entreprises ne seront pas les mêmes pour couvrir une opération de qualimétrie du code que pour gérer le déploiement d'un \gls{hotfix}.

\paragraph{Gestion de branches}

La gestion de branches est importante à bien définir puisqu'elle va rythmer les flux de travail de développement des équipes. Il n'est donc pas envisageable de changer les habitudes de travail des équipes toutes les 3 semaines sous prétexte que ce flux de travail sera plus adapté avec les outils d'intégration ou de déploiement continu.

On peut alors définir des conventions de nommage des branches. Par exemple, on peut utilises une branche \emph{master} qui représente l'état de la version de production et une autre branche \emph{develop} dite de développement sur laquelle les \emph{Dev} travaillent. Une branche de \gls{hotfix} peut être conventionnellement nommée \emph{hotfix\_NUMERO\_INCIDENT} par exemple pour gérer les incidents de production. Une fois ces conventions mises en place, il sera alors plus facile de déterminer différents comportement dans les outils d'intégration et de déploiement continu, tel que Jenkins à titre d'exemple.

\emph{Au niveau des branches, de nombreuses conventions existent, ci-dessous est présenté un exemple.}

Le feature branching, aussi connu sous le nom de \frquote{git flow} consiste à organiser son processus de développement sous forme de branche, en faisant correspondre à chaque fonctionnalité une branche. On conserve le principe de branche \emph{master} et \emph{develop} présentée dans le paragraphe précédent, mais à chaque fois que l'on souhaite développer une fonctionnalité, on créé une branche en se basant depuis \emph{develop}, ce qui permet de laisser les branches de développement et de production relativement saine. Une fois le développement de la fonctionnalité terminée, il suffit alors de \emph{merger}\footnote{fusionner le code original et le code développé sur la branche de fonctionnalité} les deux branches pour récupérer le travail.

\newImage{0.2}{feature-branching.png}{Illustration du feature branching - \url{https://www.atlassian.com}}{feature-branching}

Le feature flipping n'est pas à proprement parler un flux de travail puisqu'il peut très bien s'accorder sur un flux de travail tel que le feature branching ou sur une branche simple mais il est intéressant de le mentionner. Il consiste à envelopper chaque fonctionnalité avec un paramètre de configuration, de sorte à pouvoir activer et désactiver rapidement des fonctionnalités. Cela peut permettre de tester des nouvelles fonctionnalités progressivement vis-à-vis des utilisateurs ou d'effectuer du \gls{test-a-b} afin de déterminer quelle solution est la plus plébiscitée par les utilisateurs. 

Un exemple de feature flipping peut être le déploiement de la nouvelle interface de Gandi\footnote{\url{http://gandi.net/}} qui a progressivement proposé sa nouvelle interface\footnote{\url{https://news.gandi.net/fr/tag/gandiv5/}} à ses utilisateurs via un bouton leur permettant de tester la nouvelle interface, tout en leur autorisant un retour à l'ancienne s'ils le souhaitaient. 

\paragraph{Gestion de version}

Une fois que l'on a déterminé la façon dont le code source sera organisé, il est nécessaire de déterminer la façon dont les versions du logiciel seront effectuées. En effet, il faut être en mesure de pouvoir dire quelle version est déployée sur quel environnement. Il est rare de disposer seulement d'une production, on dispose à minima pour un projet d'un environnement de recette et d'une pré-production, et souvent d'un contexte de staging pour pouvoir valider les modifications à différentes étapes et éviter de se retrouver avec un bug en production. Ainsi, il n'est pas rare d'avoir une version A en production, avec une version B en pré-production et une version C en recette client.

La version doit être immuable, c'est à dire qu'il ne doit pas être possible de déclarer deux fois la même version. Une fois déclarée, la version doit être taguée dans le \gls{SCM} afin d'en garder une trace et de la retrouver si besoin. Concernant le format de la version, il doit pouvoir être automatiquement incrémenté par les outils d'intégration et de déploiement continus. On recommande généralement alors soit un format de version via la date (\emph{2019-06-17.1} pour la première version du 17/06/2019 par exemple) ou un format standardisé, tel que le Semanting Versionning\footnote{\url{https://semver.org/}} dont le but est d'indiquer via le numéro de version les compatibilités entre version. Ainsi, une version 1.0.0 et une 2.0.0 peuvent entrainer des non compatibilités ascendante ou descendante. Néanmoins, une 1.0.0 et une 1.1.0 indique l'ajout de fonctionnalités n'entrainant pas de non-compatibilités et la différence entre une 1.0.0 et une 1.0.1 indique l'ajout d'un correctif mineur.

\paragraph{Gestion des opérations}

Une fois la façon dont le code est géré par le \gls{SCM} et la façon dont on nomme nos versions, on peut se poser la question sur les différentes actions à effectuer. On va en effet vouloir effectuer des actions rapide suite à une modification du code pour pouvoir valider que ce code est fonctionnel (en lançant les tests unitaire par exemple) mais on ne va pas vouloir forcément produire immédiatement la qualimétrie du code, qui peut prendre du temps à être générée et donc faire attendre les équipes de développements qui attendent un retour rapide. De même, selon les clients, on va vouloir déployer automatiquement un projet s'il passe les tests d'intégration et les tests système, mais certains projets anciens n'ayant pas forcément assez de tests ne peuvent peut-être pas se permettre un déploiement immédiat et il faudra alors demander une validation manuelle.

Les opérations sont à prévoir pour tous les cas pouvant intervenir, qu'il s'agissent de déploiement d'une nouvelle version, d'un \gls{hotfix} ou de la construction de dépendances internes étant réutilisés par d'autres applications (voir annexe \ref{annexe:commons-modules}).

On peut consulter sur l'annexe \ref{annexe:release-naq} le diagramme des opérations pour le déploiement d'une nouvelle version d'un site de la \naq{} qui passera par 4 environnements, un de staging, suivi d'une recette effectuée par le client, puis d'une pré-production et enfin de l'environnement de production. On constate que le serveur d'intégration continu (ici Jenkins) est au cœur des opérations effectuées et sert de chef d'orchestre pour interagir avec les autres outils et environnements. 

Une autre notion que l'on peut voir sur ce schéma est la notion de promotion d'artifact. Un artifact représente un fichier (dans ce cas, une archive contenant le site à déployer) stocké sur un \gls{artifactory}. Ce fichier va, au cours du \emph{pipeline}\footnote{Flux de travail}, être validé selon qu'il a ou non été correctement déployé sur un environnement donné. On va alors promouvoir ce fichier selon des statuts défini arbitrairement afin de situer l'évolution du fichier. Les statuts définis dans ce pipeline sont les suivants : \frquote{staging}, \frquote{RC}\footnote{Release Candidate, correspondant à l'environnement de recette client} et \frquote{production}\footnote{dans l'annexe, production est implicite}. On considère alors que si le package atteint l'environnement de pré-production, il peut directement être promu en tant que version définitive.

Il y a également un dernier aspect important concernant la gestion des opérations lorsque l'on souhaite mettre en place une démarche \devops : Il s'agit de ne pas viser trop grand dès le début. On peut en effet tout automatiser (montée de version des dépendances automatique, déploiement et provisionnement d'environnement de tests automatiquement\ldots) mais en fonction du besoin et du budget client, tout ne sera pas possible. Il vaut peut-être mieux commencer par ne faire que de l'intégration continue, sans livraison ou déploiement continue, que vouloir construire une chaine d'industrialisation tellement complexe qu'elle ne sera utilisée par personne et aura coutée plus cher que le développement du projet. La matrice en annexe \ref{annexe:devops-matrice} présente bien cela, avec divers niveaux, représentant différentes étapes vers la mise en place d'une démarche \devops.

\subsubsection{Choix des outils}

Une fois que l'on sait comment gérer ses branches, ses versions et que l'on sait les différentes opérations à effectuer pour les différents cas d'usages, il convient de sélectionner les bon outils afin de mener à bien le projet.

Un \gls{SCM} est évidemment obligatoire pour pouvoir effectuer une intégration continue des changements du code, taguer les différentes versions ou encore permettre de séparer le travail en cours avec différentes branches. Plusieurs solutions existent, certaines propriétaires, d'autres open-source ou auto-hébergeables. Parmi les plus populaires, on peut citer \github\footnote{\url{https://github.com}}, Gitea\footnote{\url{https://gitea.io}}, solution auto-hébergeable et économe en terme de ressources ou encore Gitlab\footnote{\url{https://gitlab.com}} qui est utilisé dans le cas des projets de la \naq, auto-hébergé au sein de \onepoint.

Chaque projet va souvent dépendre de dépendances. Dans le cas d'un projet \gls{PHP}, on va souvent retrouver des dépendances via composer, qui sont des dépendances externes. Mais on peut également dépendre de dépendances internes, qui peuvent être des composants réutilisés en interne au sein de plusieurs projets. Il faut alors pouvoir gérer ses dépendances. S'il est possible de déployer ses dépendances internes sur Packagist\footnote{Gestionnaire de dépendances \gls{PHP} - \url{https://packagist.org/}}, il se peut que l'on souhaite garder la main sur ses dépendances, ou ne pas les rendre publique, et que l'on souhaite donc les garder sur un gestionnaires de paquets privés. Dans le cas de la \naq, c'est l'\gls{artifactory} de Jfrog\footnote{Gestionnaire de fichier binaire - \url{https://jfrog.com/artifactory/}} qui est utilisé afin de gérer ses dépendances. Concrètement, cela consiste à ajouter un repository\footnote{dépôt} au fichier de définition des dépendances (dans le cas de \gls{PHP}, il s'agit d'un fichier appelé \frquote{composer.json}).

%\begin{listing}[H]
%	\begin{minted}{json}
%	{
%	"type": "composer",
%	"url": "https://ARTIFACTORY/api/composer/nouvelle-aquitaine",
%	"vendor-alias": "nouvelle-aquitaine"
%	}
%	\end{minted}
%	\caption{Définition d'un dépôt composer externe dans un fichier %\frquote{composer.json}}
%	\label{code:composer-setup}
%\end{listing}

Il est ensuite nécessaire, pour pouvoir effectuer de l'intégration continue, de disposer d'un serveur\ldots{} D'intégration continue. De nombreux outils existent, chacun se démarquant avec ses propres fonctionnalités. Les plus connus sont TravisCI\footnote{\url{http://travis-ci.org/}} qui est souvent utilisé lors de projets étant versionné sous \github, TeamCity\footnote{\url{https://www.jetbrains.com/teamcity/}} qui après ses \gls{IDE}, propose également son propre outil d'intégration continue. Finalement, le leader reste Jenkins\footnote{\url{http://jenkins.io/}}. Tous permettent de réaliser de l'intégration continue, mais proposent souvent plus, tel que la livraison ou encore le déploiement continu, le tout selon les besoins \& le budget du client.

\subsubsection{Intégration continue}

% AKA build continu
% Séparer en deux jobs : un après push + un par jour qui fait la qualimétrie.

\todo{Intégration continue}

Le but de l'intégration continue est d'intégrer les changements des \emph{Dev} et de les tester, à chaque nouvelle modification du code. Pour cela, on utilise un logiciel s'exécutant sur un serveur, connecté au gestionnaire de sources contenant le code. Dès qu'une modification de code est poussée sur le gestionnaire de source, un \gls{webhook} est envoyé à l'outil d'intégration continue (par exemple Jenkins \cite{jenkins-guide}) qui va alors dérouler un certains nombre d'étapes définies (installation des dépendances, exécutions des tests\ldots) pour vérifier que l'application est conforme aux demandes client. Ces retours doivent être rapide afin que les développeurs puissent s'assurer qu'il n'y a pas d'erreurs bloquante dans leur code.

On peut alors découper l'intégration continue en deux étapes. La première serait \textbf{le build continu}, qui consiste à fournir un retour rapide aux développeur. Son temps d'exécution doit être de l'ordre de quelques minutes maximum, c'est pourquoi on se contente souvent de lancer seulement les \gls{TU}. On retrouve l'illustration de ce flux de travail dans l'annexe \ref{annexe:build-continu}.

Une autre étape importante de l'intégration est la \textbf{qualimétrie}. Il s'agit d'un job qui va être exécuté une fois par jour, en général la nuit\footnote{lorsque l'activité sur le projet est faible} et qui va construire le projet en récupérant les dernières sources disponibles sur la branche de développement, exécuter les tests unitaire, des tests de qualimétrie, tel que des outils de détection de code copier coller qui pourrait être plus difficile à maintenir\footnote{En \gls{PHP}, il existe PHPCPD qui répond à ce besoin - \url{https://github.com/sebastianbergmann/phpcpd}} et on va exécuter les tests d'intégration, qui peuvent nécessiter le provisionnement d'une base de données et être plus long à exécuter que les \gls{TU}. Une fois tous ces tests effectués, on va générer un rapport et le mettre à disposition des équipes de développement qui pourront le consulter le lendemain à leur retour et prendre en compte les remarques du rapport.

% Métriques pour attester déroulement OK => test OK/KO, temps traitement. 

\subsubsection{Livraison continue}

\todo{Livraison continue}

Une étape suivant l'intégration continue est la livraison continue. Cette dernière consiste à construire l'application, ses dépendances et à continuellement construire un binaire (souvent une archive zip ou tar.gz) qui va permettre le déploiement de l'application.

Livre du Build des package + test continu dès leur validation. Système de promotion environnement manuel.

Métriques pour attester déroulement OK => Taille paquet, temps traitement. 

Permet déploiement via interaction humaine.

\textit{Exemple : parler de NAQ Automatisation de la création de la base de données et de la restauration des données}

\subsubsection{Déploiement continu}

\todo{Déploiement continu}

deploiement de packages continu dès leur création et validation. Système de promotion auto.

Métriques pour attester déroulement OK => Uptime, temps déploiement, ratio rollback suite à déploiement foiré\ldots

scalabilité, résilience. provisionner de nouveaux serveurs rapidement avec Ansible par exemple. Inconvénient : nécessite une architecture découplée, si on vient a faire un changemnt (ex: rajouter un serveur redis) mais qu'on doit attendre 2 semaines pour valider ce changement, on pert intérêt

\subsubsection{Gestion de l'infrastructure}

\todo{Gestion de l'infrastructure}

\gls{IaC} => permet de découpler architecture ? % https://dev.to/klauenboesch/why-use-infrastructure-as-a-code-3793

% https://aws.amazon.com/fr/cloudformation/ ----------- https://www.terraform.io/

\subsubsection{Monitoring}

\todo{Monitoring}

% https://prometheus.io/

Le reporting de bug automatique (sentry.io)

Monitoring : monit pour monitorer l'état d'application ? (perso)

ELK, traitement logs, grafana\ldots

\subsection{Et la sécurité dans tout ça ?}

Un projet automatisé de A à Z peut être testé intégralement, si la sécurité autour de l'infrastructure d'intégration et de déploiement n'est pas sécurisée, cela revient à avoir une sécurité digne d'une porte ouverte.

En effet, si l'on vient à déployer de façon automatique, il va falloir que les outils déployant et testant automatiquement l'application ait accès aux données de l'application, données qui peuvent parfois contenir des tokens, des clés d'\gls{API}\ldots Il faut alors réfléchir à la façon de sécuriser l'infrastructure. 

Il faut tout d'abord restreindre les accès aux différents outils d'automatisation. Pour cela, on peut se servir d'un pare feu, qui va filtrer les connexions entrantes sur les différents serveurs. On peut également mettre en place une \gls{DMZ}, qui permet ainsi de séparer le réseau interne de celui externe, connecté à Internet et qui contient les applications ayant besoin d'être accessible via l'extérieur. En plus de cela, on peut également restreindre les accès aux différentes applications via la mise en place d'un \gls{VPN}, qui permet l'accès aux applications via une connexion sécurisée, comme si l'on était présent sur site et évite ainsi d'avoir à exposer les applications sur Internet. Mais toutes ces mesures sont valable pour tout type d'application, automatisée ou non. 

On peut (\emph{et l'on doit}) aussi se référer à la documentation des outils d'automatisation utilisés. Par exemple, la documentation de Jenkins\footnote{\url{https://jenkins.io/doc/}} ainsi que le livre numérique \citetitle{jenkins-guide} \cite[chapitre 7, Sécuriser Jenkins]{jenkins-guide} décrivent les façons de sécuriser l'outil et éviter ainsi que le serveur d'intégration ne serve de vecteur d'attaque pour compromettre l'application (\emph{ou pire, l'entreprise entière, étant donné qu'un système d'intégration continu est souvent utilisé pour plusieurs projets}). Une des premières mesures est de restreindre les accès utilisateurs, en appliquant le principe de moindre privilège. Cela signifie donner uniquement les droits strictement nécessaire. Si un utilisateur a simplement besoin de consulter les rapports, il n'y a pas d'intérêt de lui donner les droits pour pouvoir éditer la configuration de ces derniers.

Concernant les interactions entre le serveur d'intégration continue, le gestionnaire de version et les différents serveurs, il peut être intéressant de créer un compte de service (\emph{compte système, n'étant relié à aucun utilisateur et disposant des droits minimaux pour l'exécution de la tâche pour lequel il est utilisé}) avec des accès par clé \gls{SSH} utilisés pour les déploiements. L'utilisation d'une clé \gls{SSH} plutôt qu'un mot de passe est nécessaire puisque la demande de mot de passe se fait de façon interactive (et requiert donc une intervention utilisateur)\footnote{Même s'il est possible de contourner ce fonctionnement, par exemple avec sshpass, il est recommandé d'utiliser des clés \gls{SSH}}. De même, l'accès aux serveurs doit être limité et surtout contrôlé. Il faut ainsi préférer des comptes nominatif, même si leur création peut s'avérer plus fastidieuse plutôt qu'un seul compte administrateur partagé entre les membres de l'équipe. Ainsi, si un compte vient être compromis, il est plus facile de révoquer ses accès plutôt que de bloquer l'accès à tous les membres de l'équipe.

Un autre aspect à prendre en compte, selon la confidentialité requise par les projets, est l'endroit ou le code source du projet va transiter. Si par exemple, un prestataire ne dispose pas de son propre serveur d'intégration continue mais possède une offre chez un autre prestataire (TravisCI par exemple), il faut être prudent, puisque le code sera alors à un moment transmis en dehors du réseau de l'entreprise. Selon les projets, il peut être nécessaire d'auto-héberger des solutions en interne afin de garder une maitrise totale de la chaine d'industrialisation.

Finalement, il ne faut pas oublier la vérification de la sécurité de l'application elle-même. En effet, une application nécessite souvent des dépendances externes, réalisées par d'autres personnes. Comment s'assurer que ces dépendances sont à jour et fiable ? 

On peut ainsi utiliser les outils de gestion de dépendances afin de s'assurer que les dépendances sont à jour. Cela requiert néanmoins une action manuelle de la part du développeur. Des outils existent donc afin de surveiller les mises à jour des dépendances, tel que Depfu\footnote{\url{https://depfu.com/}} ou encore DependABot\footnote{\url{https://dependabot.com/}} et permettent de s'intégrer directement la chaine d'intégration continue en proposant des modifications sur une branche dédiée, basée sur la branche de travail principale du dépôt du gestionnaire de version, ce qui permet de tester ces modifications avant de les intégrer dans l'application.

%\begin{listing}[H]
%	\begin{minted}{bash}
%	#!/bin/bash
%	# Liste les dépendances non mises à jour
%	npm outdated # Javascript
%	composer outdated # PHP
%	bundle outdated # Ruby
%	\end{minted}
%	\caption{Exemple de vérification des dépendances non mises à jour}
%\end{listing}

\todo[color=cyan]{Sécurité, 1-2 page }

OWASP ZAAP

bastion qui peut permettre de couper l'accès à l'infra.