\subsection{Gestion de projet}
\todo[color=orange]{Renommer cette partie}

\epigraph{\frquote{Avoir 300 personnes dans une cuisine pour faire un œuf au plat ne permettra pas de servir le plat 300 fois plus vite}}{Analogie de la \frquote{Loi de Brooks\footnote{\url{https://fr.wikipedia.org/wiki/Loi_de_Brooks}}}}

\subsubsection{Partie financière}
L'automatisation d'un projet ne peut s'effectuer un en jour. De même que la citation ci-dessus fait sens, la migration d'un projet existant vers une automatisation totale ne peut s'effectuer en un jour, même en mobilisant de grand moyens. Il est donc nécessaire de préparer et planifier une conduite du changement qui permettra d'accompagner tous les acteurs du projet, afin que la transition s'effectue dans les meilleurs conditions possible. Cela passe d'abord par une réflexion sur la conduite à adopter et les changements à apporter. 

Prenons pour exemple un projet fonctionnel depuis maintenant plusieurs mois. Le projet a déjà été livré (avec plus ou moins de difficulté) quelques fois en production. Comment convaincre que l'automatisation va apporter du bien à ce projet ? Qui est-il nécessaire de convaincre ?

Tout d'abord, il faut convaincre les personnes responsables financièrement du projet. En effet, il est possible d'avoir un projet automatisé de A à Z, si ce processus d'automatisation vient à coûter 3 fois le prix total vendu au client, aucun responsable ne vous donnera son accord. Il va également falloir convaincre les personnes responsables du projet (chef de projet, directeur de projet...) qu'il y a un intérêt à effectuer cela. En effet, sans garantie que ce processus pourra amener quelque chose de positif, peu laisseront une opportunité d'essayer d'automatiser un projet.

Il faut ainsi préparer son argumentaire, avec les bienfaits que cela pourra obtenir sur les \gls{KPI} et sur la vie du projet à moyen/long terme.

\begin{description}
	\setlength\itemsep{0em}
	\item [La durée de de déploiement] peut être réduite pour permettre de se concentrer sur d'autres problématiques. De plus, si les temps de déploiement sont réduits, cela signifie que les temps d'interruption de service pourront être réduit lors des livraisons.
	\item [Erreur de déploiement] Si les déploiements sont automatisés, les risques d'incidents lors des déploiements, souvent du à des erreurs humaines, seront alors réduit.
	\item [Rentabilité financière] Prenons une tâche qui coute \numprint{3000}€ , à réaliser toutes les 3 semaines. Son cout est donc assez élevé. Le fait d'automatiser cette tâche permettra d'économiser de l'argent sur la durée. Si le fait d'automatiser cette tâche coute par exemple, \numprint{10000}€, un gain sera perçu dès la 12\up{ème} semaine.
	\item [Part de marché] Si les déploiements sont plus fiable et rapide, que la rentabilité financière du projet est également florissante alors il sera possible de concentrer les efforts sur des fonctionnalités à forte valeurs ajoutées qui permettront ainsi de se démarquer de la concurrence et d'augmenter les parts de marché à moyen/long terme.
	\item [\Gls{timetomarket}] Dans un environnement très concurrentiel, il est essentiel de pouvoir intervenir rapidement. Réduire la durée entre la naissance d'une idée/besoin et sa mise en production est important, sous peine de voir des concurrents prendre des parts de marché. L'automatisation peut alors permettre de déployer rapidement et avec confiance les développements réalisés.
	\item [Reprise d'activité] Il s'agit ici de démontrer que le produit parfait n'existe pas. Il arrivera que l'application soit hors ligne à cause d'un incident ou qu'une mise à jour entraine une interruption de service. En revanche, si l'on ne peut éviter l'incident à un moment donné, il est capital de pouvoir restaurer l'application dans des court délai et ce de façon certaine. Disposer d'un \gls{PCA} ou d'un \gls{PRA} intégrant une automatisation de la reprise d'activité est un atout non négligeable qui permettra une reprise de l'activité au plus vite. On peut alors passer d'une interruption de plusieurs heures à une se comptant en minutes.
\end{description}

\subsubsection{Organiser le flux de travail}
Il est également nécessaire de planifier au mieux les durées des travaux à accomplir afin d'éviter des dépassements dans les délais annoncés. Cela implique donc de réfléchir au temps nécessaire à investir et aux différents jalons qui valideront la bonne mise en place de l'automatisation. 

Pour pouvoir mettre en place une démarche d'automatisation sur un projet, il est nécessaire que ce dernier soit assez mature pour pouvoir s'y adapter. Ainsi, comme l'indique \citetitle{phoenixProject} \cite{phoenixProject},  avant de se lancer dans une démarche d'automatisation (\emph{ou plus globalement, pour pouvoir gérer correctement un projet}), la première étape est de savoir gérer le flux de travail.

Le flux de travail, ce sont les différentes tâches qui sont à réaliser sur le projet. Ces dernières sont souvent des stories (\emph{dans le cas d'une méthodologie agile}), définies conjointement avec le client et correspondent ainsi à ce qui a été défini dans le cahier des charges. L'inconvénient est que ce flux de travail est fragile. Il peut rapidement dériver et se retrouver surcharger, surchargeant ainsi les équipes. Le principal ennemi de flux de travail est le travail non prévu (\emph{unplanned work}). Ce sont toutes les petites tâches non prévues, qui prennent 10 à 15 minutes, qui ne sont suivies nulle part et qui mises bout à bout peuvent faire perdre plusieurs jours de travail et épuiser les équipes, qui ne voit jamais le travail diminuer.

Quelles sont donc les solutions pour gérer ce travail non prévu ? L'objectif est ici de faire en ce sorte que ce travail non prévu ne soit plus exécuté directement sans passer par une phase minimale de planification. Pour cela, il est possible de mettre en place des kanbans. Qu'est ce qu'un Kanban ? Il s'agit d'une méthode de représentation des tâches qui permet de visualiser l'avancée du projet en temps réel (\emph{ou du moins, à chaque fois que le tableau est mis à jour}). 

\newImage{0.35}{kanban.jpg}{Représentation d'un tableau Kanban -  \url{https://commons.wikimedia.org/}}{kanban}

On peut constater sur la figure \ref{fig:kanban} que l'on peut limiter le travail non prévu, en l'ajoutant à une catégorie, le \emph{backlog}, qui correspond à la pile de tâche à réaliser. Une fois ces tâches estimées, découpées et discutées avec le client, elles peuvent être considérées comme prête à être commencées. On parle alors de \emph{Définition Of Ready}. De même, pour déterminer qu'une tâche a été terminée par l'équipe de développement et donc prête à être livrée, on parle de \emph{Definition Of Done}, ce qui correspond à une liste de points, établi par l'équipe en interne. Ces points correspondent à des éléments à vérifier et à cocher pour indiquer si une tâche est vraiment terminée ou non. On peut y retrouver par exemple l'écriture de tests ou encore le fait qu'une autre personne ai relu le code, le fait que la documentation ait été mise à jour\ldots

On peut ensuite suivre l'évolution des tâches dans leur différentes étapes, du développement à leur livraison, en passant par les tests. Ainsi, le travail non prévu ne peut plus déranger un sprint en cours puisqu'il est possible de définir des limites, en disant par exemple que seules les tâches étant prêtes seront travaillées dans ce sprint et que les autres devront attendre le prochain sprint pour pouvoir être traitées. Néanmoins, il s'agit de faire attention à un point en mettant en place un kanban. En effet, si l'on met simplement en place un kanban et que l'on continue de traiter les tâches comme elles l'étaient précédemment, on s'expose au risque de voir le travail en cours, ou \emph{WIP\footnote{Work In Progress}} s'accumuler. Cela risque donc de ralentir les équipes voir de les démotiver. De plus, si tous le travail est en WIP, il n'y a aucune façon de savoir depuis combien de temps un tâche est présente ici, ce qui fait que certaines tâches peuvent très bien rester en WIP pendant des jours voir des semaines. La solution à ce problème tient dans le fait de limiter le travail en cours. En effet, en limitant le travail en cours, on s'assure que les tâches ne resteront pas éternellement en attente. Les tâches bloquées pourront éventuellement être passées dans une colonne "Bloqué"\footnote{Cette colonne devra elle aussi être limitée en éléments, sous peine de seulement décaler le soucis !}. De plus, si le travail en cours est restreint, il sera de meilleur qualité, puisque moins dispersé et donc engendrera moins de bug !

\subsubsection{Livrer plus rapidement de la valeur métier}
L'un des objectifs de l'automatisation est également de faciliter le passage d'un système où l'on met plusieurs mois avant de livrer une grosse fonctionnalité à un autre ou l'on livre de multiples petites fonctionnalités. Attendre plusieurs mois pour livrer une fonctionnalité peut en effet avoir des effets indésirables.

\begin{itemize}
	\setlength\itemsep{0em}
	\item Dans une époque hyper-concurrentielle où l'on veut tout, toujours plus vite, certains besoins sont éphémères ou nécessite un \gls{timetomarket} faible pour se démarquer de la concurrence. Si l'on livre la fonctionnalité une fois le besoin passé, la fonctionnalité n'a plus de sens. Cela vaut aussi si l'on vient à livrer un produit une fois que tous les concurrents sont déjà positionnés dessus. Il vaut mieux livrer petit à petit des parties du besoin pour pouvoir l'améliorer continuellement.
	\item Une fonctionnalité qui met longtemps à être livré induira un stress au niveau des équipes, qui auront sur leurs épaules la responsabilité de livrer \emph{la fonctionnalité parfaite}, plutôt que livrer plusieurs petites fonctionnalités
	\item Le risque d'échec est beaucoup plus important en livrant une fonctionnalité plutôt que plusieurs petites, surtout si des changements d'architecture (\emph{Structure de la base de données, ajout de nouveaux composants...})
	\item Plus l'on attend longtemps pour livrer une fonctionnalité, plus on prend le risque de se retrouver dans le cas d'un \emph{effet tunnel} ou la compréhension de ce qui était à faire s'est retrouvée au fur et à mesure du temps par être l'opposé de ce qui était demandé. Dans cette position, le client et le chef de projet communique souvent peu, l'équipe de développement travaille en général sans avertir de ses avancées,\ldots Seule la date limite permet de voir ce qui a été réalisé et il s'agit plus souvent d'un échec que d'une réussite.
\end{itemize}

Cette volonté de vouloir livrer plus petit mais plus régulièrement s'inscrit dans \emph{l'amélioration continue}. Au début, la fonctionnalité ne sera peut-être pas parfaite, mais elle sera continuellement améliorée jusqu'à correspondre au besoin voulu. 

\newImage{1}{devops-objective.jpg}{Mise en place de l'amélioration continue entre \emph{Dev} et \emph{Ops} - \url{https://itrevolution.com}}{devopsObjective}

Néanmoins, il convient de vérifier que ce système est intégré et accepté par l'équipe, sous peine de voir un rejet de cette méthodologie et d'entrainer des conflits internes dans les équipes. Cela peut être le cas avec certaines personnes réfractaires au changement, par exemple des développeurs âgés, proche de le retraite, qui n'ont peut-être pas envie de se reformer et s'adapter à de nouvelles méthodes de travail et qui tenteront de garder leurs habitudes. Cela peut également provenir de personnalités avec un fort caractère qui pensent que leur méthode est plus efficace que les autres ou encore de bien d'autres cas. De manière générale, les personnes ne souhaitent pas sortir de leur zone de confort. Pour permettre ce changement, il va falloir exposer les différents avantages aux équipes et leur montrer les intérêts qu'ils ont à s'essayer à cette méthode pour peut-être finir par l'adopter de leur plein gré. \todo[color=cyan]{Devops handbook p.59 parler de convaincre d'abord early adopter, puis une majorité silencieuse (suiveur) et ensuite de convaincre les récalcitrant.}

\subsubsection{Métriques et apprentissage continu}

L'un des premiers avantages si l'on met en place une démarche d'automatisation est une meilleure communication entre les développeurs, également appelés \emph{Dev}, et les administrateurs systèmes, appelés \emph{Ops}. Si l'on met en place une démarche d'automatisation, un besoin de métrique va être nécessaire. Ces métriques, semblables aux \gls{KPI} précédemment détaillés, permettront de savoir par exemple le temps de déploiement de l'application, les derniers bugs remontés, les suivis de performance afin de déterminer si une requête \gls{SQL} est trop lente ou non... Ces métriques, conjointement définies par les \emph{Devs} et les \emph{Ops} vont permettre d'obtenir des retours rapides sur l'état de l'application et ainsi de pouvoir améliorer le produit plus facilement, en corrigeant les bugs au plus tôt. C'est la base de la \emph{Second Way}, ou deuxième étape, comme expliqué dans \citetitle{phoenixProject} \cite[p.405-410]{phoenixProject}.

Un autre point à prendre en compte lors d'une démarche d'automatisation est l'apprentissage continu. Comme nous l'avons vu dans la partie précédente, \etsy a mis en place un outil de post-mortem (p.\pageref{post-mortem}) qui permet de savoir les erreurs rencontrées dans le passé et de permettre qu'elles ne se reproduisent plus. Cette notion constitue la \emph{Third Way} mais ne se limite pas seulement aux erreurs rencontrées. Avec le temps obtenu par l'automatisation, il est ainsi possible de s'améliorer sur d'autres points : partager les connaissances de chacun pour faire progresser l'équipe et les personnes la constituant, documenter et améliorer l'intégration de nouvelles personnes dans les équipes ou les environnements de travail\ldots{} L'équipe sera alors plus performante et pourra ainsi délivrer de meilleurs produits.

%Le temps libéré par l'automatisation des tâches peut permettre de souder les liens d'une équipe et d'améliorer les relations de cette dernière. Cela libère du temps pour du team building par exemple. Un sujet technique qui rapproche en terme d'humain

\subsection{Environnement de développement}

Objectif

\subsubsection{Arrivée sur le projet}

Temps d'efficience => installation \& co

\subsubsection{Build local}

- IDE

- MAkefile script installation, aide tâche récurrente\footnote{Et pourquoi pas même faire le café ? (cf. \url{https://github.com/NARKOZ/hacker-scripts})}

- SCM (hook)

- Pair programming / code review

- Test ( TDD /TCR test commit revert / ...)

\todo[color=cyan]{Localement, lors des phases de développement, 2 pages}

Permettre un démarrage rapide pour pouvoir se concentrer sur une fonctionnalité plutôt que sur le "ça marche pas parce que tu n’as pas installé ça". => Valeur ajoutée, performance, productivité

Déploiement : chaine de déploiement (dev/test/inte/preprod/prod) avec chacune ses spécificités

pair programming => gain temps car détection erreur au plus tôt.

Exemple :

en dev, on souhaite des logs direct dans la console, en prod on les mets dans un fichier de log.
en dev, on veut le mode debug, en prod on le désactive.

rapprocher env de dev et de prod.

But : prévenir l'erreur avant qu'elle ne quitte le poste du développeur (et soit donc sur le dépôt Git).

-  Linter

-  Docker, stack reproductible et indépendante selon environnement (windows/linux/mac) 

- IDE à configurer, éventuellement partager la configuration (ex: PHP Storm Code Style)

-  Makefile / scripts shell de taches récurrentes (cache clear, installation...)

- Git hook pour vérifier que tout est OK.

mise en place code review => erreur au plus tôt
% https://dev.to/shameemreza/3-tricks-to-automate-development-tasks-with-git-hooks-2dah

\subsection{L'importance des tests} \label{importance-test}

Les tests ont un rôle crucial dans une démarche d'automatisation. En effet, lorsque l'on automatise quelque chose, il est essentiel de s'assurer que le comportement de l'application est valide puisque l'on va par la suite déployer automatiquement cette application en production. Selon le \gls{CFTL} et l'\gls{ISTQB}, il existe 4 types de tests et chacun dispose de 4 objectifs formant ainsi une matrice représentée dans le tableau \ref{tab:matrice-test}.


\begin{table}[H]
	\centering
	\begin{adjustbox}{width=0.95\textwidth,center=\textwidth} 
		\begin{tabular}{|l|l|l|l|l|}
			\hline
			\diagbox{type de test}{valide} & Fonctionnement unitaire & Règle métier & Développement & Interaction \\ \hline
			Test de composant     & X                       & X            & X              &             \\ \hline
			Test d'intégration    &                         & X            & X              & X           \\ \hline
			Test Système          &                         & X            &               & X           \\ \hline
			Test d'acceptation    &                         & X            &               &             \\ \hline
		\end{tabular}
	\end{adjustbox}
	\caption{Matrice de tests selon leur type et leur objectif}
	\label{tab:matrice-test}
\end{table}

\todo{Valider cette matrice car elle est sûrement fausse?}

\subsubsection{Le test de composant}\label{test-composant}

Aussi appelé \gls{TU}, son objectif est que le composant testé soit totalement isolé afin de se concentrer uniquement sur le \emph{fonctionnement unitaire} de ce dernier, sans dépendre d'autre composant. On peut alors tester tous les cas, du cas dit \frquote{général}, qui représente souvent 80\% des cas, jusqu'aux \frquote{cas limites}, cas apparaissant plus rarement mais pouvant entrainer un comportement non souhaité de l'application. Cela permet de vérifier que le code écrit par le développeur lors de la phase de \emph{développement} est bien valide. Ainsi, un test de composant peut valider des \emph{règles métier} et ainsi également correspondre à un test d'acceptation\footnote{détails sur les tests d'acceptation dans la sous partie \frquote{\nameref{test-acceptation}} (p. \pageref{test-acceptation})}.

Un exemple commun de test de composant est par exemple le test d'une méthode de calcul d'une calculette. On dispose d'une méthode \frquote{calculer} disposant de 3 paramètres, 2 \frquote{opérandes} numériques et un \frquote{opérateur}. Un cas général serait alors de tester que \frquote{3+2} retourne bien \frquote{5} et un cas limite serait de tester que \frquote{5/0} lève bien une exception, puisqu'il s'agit d'une opération illégale.

Dans le cas de \gls{PHP}, un exemple de \gls{framework} de test unitaire est PHPUnit\footnote{\url{https://phpunit.de/}}. Le principal avantages de ce type de test est sa rapidité : ne nécessitant aucune interaction avec des composants externes (ou bien ces composants disposent d'un \gls{mock}), l'exécution des tests est très rapide.

\subsubsection{Le test d'intégration}

Une fois chaque composant testé unitairement, il convient de vérifier que ces composants sont capable d'\emph{interagir} entre eux. En effet, dans la sous-partie précédente, il était indiqué que les composants étaient isolés les uns des autres. Néanmoins, dans de nombreux cas, il est nécessaire que des composants interagissent entre eux. On va alors alors utiliser un \gls{mock} afin de simuler le comportement d'un composant, afin de tester l'autre.

Un exemple serait une \gls{API} qui communiquerait avec une base de données afin de retourner les résultats. Lorsque l'on teste l'\gls{API} unitairement, on ne souhaite pas dépendre de la base de données. On va donc simuler les réponses que nous donnerait la base de données et envoyer ces données à l'\gls{API} qui pourra alors être testée.

Cependant, si l'on teste toute notre application de cette façon, il est impossible d'attester le bon fonctionnement de nos composants entre eux. La version de la base de données pourrait en effet être incompatible avec le code actuellement utilisé par l'\gls{API}. Si aucun test d'intégration n'est effectué, la confiance dans l'application sera basée sur le \gls{mock} de la base de données et on risque alors de se retrouver dans le cas de la figure \ref{fig:no-integration-test}, avec des composants isolés fonctionnant très bien individuellement mais incapable de s'exécuter ensemble. Cela s'illustre avec la figure \ref{fig:no-integration-test}. Dans ce cas, les tiroirs s'ouvrent très bien individuellement mais lorsqu'on les assemblent, il n'est pas possible d'ouvrir les deux tiroirs en même temps, puisque le premier bloque l'ouverture du second.

\newImage{0.4}{no-integration-tests.png}{Exemple de tests unitaire sans tests d'intégration.}{no-integration-test}

Les tests d'intégration vont donc garantir que chaque composant, déployé dans l'environnement d'exécution de l'application, est capable de communiquer de façon correcte avec ses dépendances.

\subsubsection{Le test système}

\todo{expliciter la différence entre test système et test d'intégration.}

Les tests système permettent de tester le comportement de l'application sans avoir connaissance du code. Cela permet ainsi de vérifier que l'application effectue bien ce qui est indiqué dans les spécifications demandées et que les différents composants interagissent bien entre eux. Ces tests sont également appelés \frquote{tests boite noire}. De plus, les tests systèmes permettent de vérifier que les composants interagissent correctement entre eux, au niveau global de l'application (\emph{au niveau système}) et non composant par composant.

Ce sont également les tests qui sont les plus haut niveau, puisqu'il s'exécutent avec toute l'application et ses dépendances / composant. On peut par exemple citer les tests \gls{E2E} qui sont des tests système \todo{à confirmer?} qui permettent de tester le comportement d'une application en décrivant les actions réalisées sur cette dernière (cliquer sur un bouton, réduire la fenêtre) et les résultats attendus.

Le \gls{framework} \gls{angularjs} , par exemple, permet de réaliser des tests \gls{E2E} avec Protractor\footnote{\url{https://www.protractortest.org/}}. Cela va ainsi émuler un navigateur dans lequel il va être possible de décrire les comportements utilisateurs. 

Un autre exemple de test système serait un test de charge, réalisé sur un environnement similaire à la production, durant lequel on ne va pas tester la conformité de l'application mais sa capacité à supporter un certain trafic. Dans le cas d'une application web, cela consiste à simuler des centaines ou milliers de connexion simultanées sur le site afin de vérifier sa capacité à répondre convenablement. 

L'avantage de ces tests systèmes est qu'il s'agit des tests les plus complets, puisqu'ils permettent de tester le produit fini. L'inconvénient est que ces derniers sont plus longs à exécuter.

%\subsubsection{Campagne de test}
%Parler de Protractor et Squash sur \bv
%Parler de Protractor et Squash sur \bv. Tests automatisés unitaires (phpunit) permettant à la plateforme d'intégration continue de vérifier la conformité du code par rapport aux attentes métiers.

\subsubsection{Le test d'acceptation}\label{test-acceptation}

Souvent réalisé par le client, il va consister, au travers d'une campagne de test généralement manuelle, à vérifier que les \emph{règles métiers} sont bien respectées. Le client peut néanmoins décider de déléguer ce test à une équipe qualité, ou à un prestataire externe.

Il est néanmoins possible que certain test d'acceptation soit validé en amont, comme l'exemple de la calculatrice donné dans la partie \frquote{\nameref{test-composant}} (p. \pageref{test-composant}) qui va valider la \emph{règle métier} (effectuer une opération) tout en validant le bon fonctionnement du code et ses cas limites.

\subsection{Intégration, déploiement et livraison continue}

TODO intro.

\subsubsection{Gestion de version et de branches}

Critère à définir en amont, puisque il va devoir être géré automatiquement. Intégration avec le SCM à prévoir.

Insérer références Schéma GOP. Pourquoi dump sur staging ? force utilisation update, evite clicodrome, permet test update avant prod.

feature flipping vs feature branching.


Feature flipping pour tester (A/B) et ouvrir fonctionnalité aux utilisateurs petit à petit et faire des rollback rapides.

\subsubsection{Choix des outils}

Jenkins Jfrog (artifactory) Gitlab avec interaction entre eux.

On peut tout automatiser (montée de version de deps auto chaque soir, qui va rebuild tous les projets), mais il faut faire en fonction de la durée du projet, du budget, du besoin.

Mieux vaut commencer par de l'IC simple, et éventuellement DC plutôt que direct LC. Evolutif avec le temps. De plus, le client ne veut peut-être pas de tout auto ou au contraire exige.

\subsubsection{Intégration continue}

Intégration et test des modification au fil de l'eau

Métriques pour attester déroulement OK => test OK/KO, temps traitement. 

\subsubsection{Livraison continue}

Livre du Build des package + test continu dès leur validation. Système de promotion environnement manuel.

Métriques pour attester déroulement OK => Taille paquet, temps traitement. 

Permet déploiement via interaction humaine.

\textit{Exemple : parler de NAQ Automatisation de la création de la base de données et de la restauration des données}

\subsubsection{Déploiement continu}

deploiement de packages continu dès leur création et validation. Système de promotion auto.

Métriques pour attester déroulement OK => Uptime, temps déploiement, ratio rollback suite à déploiement foiré\ldots

scalabilité, résilience. provisionner de nouveaux serveurs rapidement avec Ansible par exemple. Inconvénient : nécessite une architecture découplée, si on vient a faire un changemnt (ex: rajouter un serveur redis) mais qu'on doit attendre 2 semaines pour valider ce changement, on pert intérêt

\subsubsection{Gestion de l'infrastructure}

\gls{IaC} => permet de découpler architecture ? % https://dev.to/klauenboesch/why-use-infrastructure-as-a-code-3793

% https://aws.amazon.com/fr/cloudformation/ ----------- https://www.terraform.io/

\subsubsection{Monitoring}

% https://prometheus.io/

Le reporting de bug automatique (sentry.io)

Monitoring : monit pour monitorer l'état d'application ? (perso)

ELK, traitement logs, grafana\ldots

\todo[color=cyan]{Intégration de déploiement continu, 3-4 pages}
Une fois que les tests permettent de modifier le code de l'application avec confiance sans introduire de régression, on peut alors mettre en place une démarche d'automatisation. Cela va permettre à l'équipe de développement d'obtenir un retour rapide lors de chaque déploiement, avec un lancement automatisé des tests ainsi qu'un déploiement automatique de l'application sur un environnement de recette interne par exemple.

L'idée de l'intégration et du déploiement continu est de banaliser le déploiement afin de réduire la peur des développeurs vis à vis de la mise en production. Plus le code est déployé souvent, plus les développeurs seront serein quant à leur travail puisqu'ils pourront obtenir des retours rapide et être prévenu au plus vite lors d'incident afin de les corriger avant qu'il n'atteigne la production.

Une première étape consiste à tester de façon continue l'application, à chaque nouvelle modification du code. C'est l'intégration continue. Elle est caractérisée par un logiciel s'exécutant sur un serveur, connecté au gestionnaire de sources contenant le code. Dès qu'une modification de code est poussée sur le gestionnaire de source, un \gls{webhook} \todo{Comment setup ? webhook (avec en annexe deux images, côté Jenkins avec le secret et côté Gitlab avec le secret)} est envoyé à l'outil d'intégration continue (par exemple Jenkins \todo{citer référence vers jenkins}) qui va alors dérouler un certains nombre d'étapes (installation des dépendances, exécutions des tests\ldots) pour vérifier que l'application soit  conforme aux demandes client.   

Jenkins, Travis, Déployer automatiquement à partir d'un push sur dev, en recette, intégration, staging, et lorsque tout passe, en production.

\subsection{Et la sécurité dans tout ça ?}

Un projet automatisé de A à Z peut être testé intégralement, s'il la sécurité autour de l'infrastructure d'intégration et de déploiement n'est pas sécurisé, cela revient à ouvrir la porte à tous les maux. \todo{reformuler}

En effet, si l'on vient à déployer de façon automatique, il va falloir que les outils déployant et testant automatiquement l'application ait accès aux données de l'application, données qui peuvent parfois contenir des tokens, des clés d'\gls{API}\ldots Il faut alors réfléchir à la façon de sécuriser l'infrastructure. 

Il faut tout d'abord restreindre les accès aux différents outils d'automatisation. Pour cela, on peut se servir d'un pare feu, qui va filtrer les connexions entrantes sur les différents serveurs. On peut également mettre en place une \gls{DMZ}, qui permet ainsi de séparer le réseau interne de celui externe, connecté à Internet et qui contient les applications ayant besoin d'être accessible via l'extérieur. En plus de cela, on peut également restreindre les accès aux différentes applications via la mise en place d'un \gls{VPN}, qui permet l'accès aux applications via une connexion sécurisée, comme si l'on était présent sur site et évite ainsi d'avoir à exposer les applications sur Internet. Mais toutes ces mesures sont valable pour tout type d'application, automatisée ou non. 

On peut ainsi se référer à la documentation des outils d'automatisation utilisé. Par exemple, la documentation de Jenkins\footnote{\url{https://jenkins.io/doc/}} ainsi que le livre numérique \citetitle{jenkins-guide} \cite[chapitre 7, Sécuriser Jenkins]{jenkins-guide} décrivent les façons de sécuriser l'outil et éviter ainsi que le serveur d'intégration ne serve de vecteur d'attaque pour compromettre l'application (\emph{ou pire, l'entreprise entière}). Une des premières mesures est de restreindre les accès utilisateurs, en appliquant le principe de moindre privilège. Cela signifie donner uniquement les droits strictement nécessaire. Si un utilisateur a simplement besoin de consulter les rapports, il n'y a pas d'intérêt de lui donner les droits pour pouvoir éditer la configuration de ces derniers.

Concernant les interactions entre le serveur d'intégration continue, le gestionnaire de version et les différents serveurs, il peut être intéressant de créer un compte de service (\emph{compte système, n'étant relié à aucun utilisateur et disposant des droits minimaux pour l'exécution de la tâche pour lequel il est utilisé}) avec des accès par clé \gls{SSH} utilisés pour les déploiements. L'utilisation d'une clé \gls{SSH} plutôt qu'un mot de passe est nécessaire puisque la demande de mot de passe se fait de façon interactive (et requiert donc une intervention utilisateur)\footnote{Même s'il est possible de contourner ce fonctionnement, par exemple avec sshpass, il est recommandé d'utiliser des clés \gls{SSH}}. De même, l'accès aux serveurs doit être limité et surtout contrôlé. Il faut ainsi préférer des comptes nominatif, même si leur création peut s'avérer plus fastidieuse plutôt qu'un seul compte administrateur partagé entre les membres de l'équipe. Ainsi, si un compte vient être compromis, il est plus facile de révoquer ses accès plutôt que de bloquer l'accès à tous les membres de l'équipe.

Comme vu dans la figure \todo{ref figure setup jenkins partie intégration continue}, des tokens seront forcément nécessaire à un moment pour pouvoir garantir l'échange de données. Ces tokens sont à générer avec le minimum de droits possible (\emph{principe de moindre privilège}) et à traiter comme s'il s'agissait de mot de passe. Il ne devraient d'ailleurs être enregistrés nul part ailleurs que dans les champs de configuration requis, puisqu'ils n'ont pas vocation à être utilisés ailleurs.

Un autre aspect à prendre en compte, selon la confidentialité requise par les projets, est l'endroit ou le code source du projet va transiter. Si par exemple, un prestataire ne dispose pas de son propre serveur d'intégration continue mais possède une offre chez un autre prestataire (TravisCI\footnote{\url{http://travis-ci.org/}} par exemple), il faut être prudent, puisque le code sera alors à un moment transmis en dehors du réseau de l'entreprise. Selon les projets, il peut être nécessaire d'auto-héberger des solutions en interne afin de garder une maitrise totale de la chaine d'industrialisation.

\todo[color=cyan]{Sécurité, 1-2 page }

OWASP ZAAP

Vérifier les dépendances (failles, paquet npm corrompu...)

Des controles / supervisons périodique afin de checker que tout va bien.

bastion qui peut permettre de couper l'accès à l'infra.